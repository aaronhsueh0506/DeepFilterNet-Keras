{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from time import sleep\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from params.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from params import model_params\n",
    "p = model_params('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto, InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "# InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int64_feature(value):\n",
    "    # 轉Int64資料為 tf.train.Feature 格式\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def bytes_feature(value):\n",
    "    # 轉Bytes資料為 tf.train.Feature 格式\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_TFRecord(h5_file_clean, h5_file_noisy, output_dir, n_tfrecords = 4):    \n",
    "    # read h5\n",
    "    with h5py.File(h5_file_clean, 'r') as hf:\n",
    "        clean_data = hf['data'][:]\n",
    "        n_samples = clean_data.shape[0]\n",
    "        \n",
    "    with h5py.File(h5_file_noisy, 'r') as hf:\n",
    "        noisy_data = hf['data'][:]\n",
    "        n_samples = noisy_data.shape[0]\n",
    "    \n",
    "    h5_file = h5_file_clean.split('.')[0].rstrip('_clean') + '.h5'\n",
    "    #每個 TFR 文件放幾個檔案\n",
    "    bestnum = math.ceil(n_samples/n_tfrecords)\n",
    "    #第几个图片\n",
    "    counter = 0\n",
    "    #需要幾個 TFR\n",
    "    if n_samples%bestnum == 0:\n",
    "        num_TFR_file = n_samples//bestnum\n",
    "    else:\n",
    "        num_TFR_file = n_samples//bestnum+1\n",
    "\n",
    "    #creat output directory\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.umask(0)\n",
    "        os.makedirs(output_dir, mode=0o755)    \n",
    "    \n",
    "    # for loop to generate tfrecords\n",
    "    for k in range(num_TFR_file):\n",
    "        filename = ((h5_file.split('/')[-1]).split('.')[0] + '-{}-of-{}.tfrecord'.format(k+1, num_TFR_file))\n",
    "        TFR_gen_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        # declare TFwriter\n",
    "        TFWriter = tf.io.TFRecordWriter(TFR_gen_path)\n",
    "\n",
    "        print('\\nStart generate {}'.format(filename))\n",
    "        for i in range(bestnum):#//p.length_sec):\n",
    "            try:\n",
    "                start_idx = bestnum * k + i * 1\n",
    "                end_idx = bestnum * k + (i+1) * 1\n",
    "                \n",
    "                h5_X = noisy_data[start_idx: end_idx,:]\n",
    "                h5_Y = clean_data[start_idx: end_idx,:]\n",
    "                \n",
    "                h5_X_flatten = h5_X.flatten()\n",
    "                h5_Y_flatten = h5_Y.flatten()\n",
    "                \n",
    "#                 print(h5_X_flatten.shape)\n",
    "#                 for j in range(3):\n",
    "#                     # 將 image and label 合併成 tf.train.Features\n",
    "#                     ftrs = tf.train.Features(\n",
    "#                             feature={'X': float_feature(h5_X_flatten[j*sr:(j+1)*sr]),\n",
    "#                                      'Y': float_feature(h5_Y_flatten[j*sr:(j+1)*sr])})\n",
    "\n",
    "#                     # 將 tf.train.Features 轉成 tf.train.Example\n",
    "#                     example = tf.train.Example(features=ftrs)\n",
    "\n",
    "#                     # 將 tf.train.Example 寫成 tfRecord 格式\n",
    "#                     TFWriter.write(example.SerializeToString())\n",
    "                # 將 image and label 合併成 tf.train.Features\n",
    "                ftrs = tf.train.Features(\n",
    "                        feature={'X': float_feature(h5_X_flatten),\n",
    "                                 'Y': float_feature(h5_Y_flatten)})\n",
    "\n",
    "                # 將 tf.train.Features 轉成 tf.train.Example\n",
    "                example = tf.train.Example(features=ftrs)\n",
    "\n",
    "                # 將 tf.train.Example 寫成 tfRecord 格式\n",
    "                TFWriter.write(example.SerializeToString())\n",
    "                \n",
    "                counter = counter + 1\n",
    "                                   \n",
    "                if counter%30000 == 0:\n",
    "                    print('generating {} samples'.format(counter))\n",
    "                    sleep(0.1)\n",
    "                \n",
    "                if counter == n_samples:\n",
    "                    print('counter reach {}'.format(n_samples))\n",
    "                    break\n",
    "                    \n",
    "            except IOError as e:\n",
    "                print('Skip!\\n')\n",
    "                \n",
    "\n",
    "        TFWriter.close()\n",
    "    print('Transform done!')\n",
    "    print('total {} data processed'.format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_TFRecord(h5_file, output_dir, n_tfrecords = 4):    \n",
    "#     # read h5\n",
    "#     with h5py.File(h5_file, 'r') as hf:\n",
    "#         data = hf['data'][:]\n",
    "#         n_samples = data.shape[0]\n",
    "    \n",
    "#     print(h5_file,':',data.shape)\n",
    "#     #每個 TFR 文件放幾個檔案\n",
    "#     bestnum = math.ceil(n_samples/n_tfrecords)\n",
    "#     print(bestnum)\n",
    "#     #第几个图片\n",
    "#     counter = 0\n",
    "#     #需要幾個 TFR\n",
    "#     if n_samples%bestnum == 0:\n",
    "#         num_TFR_file = n_samples//bestnum\n",
    "#     else:\n",
    "#         num_TFR_file = n_samples//bestnum+1\n",
    "\n",
    "#     #creat output directory\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.umask(0)\n",
    "#         os.makedirs(output_dir, mode=0o755)    \n",
    "    \n",
    "#     # for loop to generate tfrecords\n",
    "#     for k in range(num_TFR_file):\n",
    "#         filename = ((h5_file.split('/')[-1]).split('.')[0] + '-{}-of-{}.tfrecord'.format(k+1, num_TFR_file))\n",
    "#         TFR_gen_path = os.path.join(output_dir, filename)\n",
    "\n",
    "#         # declare TFwriter\n",
    "#         TFWriter = tf.io.TFRecordWriter(TFR_gen_path)\n",
    "\n",
    "#         print('\\nStart generate {}'.format(filename))\n",
    "#         for i in range(bestnum//p.length_sec//100):\n",
    "#             try:                \n",
    "#                 h5_X = data[(k*bestnum)+i*p.length_sec*100:(k*bestnum)+(i+1)*p.length_sec*100,:]\n",
    "#                 print(h5_file,':',h5_X.shape)\n",
    "#                 print((k*bestnum)+i*p.length_sec*100)\n",
    "#                 # 將 image and label 合併成 tf.train.Features\n",
    "#                 ftrs = tf.train.Features(\n",
    "#                         feature={'X': float_feature(h5_X.flatten())})\n",
    "                                \n",
    "\n",
    "#                 # 將 tf.train.Features 轉成 tf.train.Example\n",
    "#                 example = tf.train.Example(features=ftrs)\n",
    "\n",
    "#                 # 將 tf.train.Example 寫成 tfRecord 格式\n",
    "#                 TFWriter.write(example.SerializeToString())\n",
    "                \n",
    "#                 counter = counter + 1\n",
    "                                   \n",
    "#                 if counter%30000 == 0:\n",
    "#                     print('generating {} samples'.format(counter))\n",
    "#                     sleep(0.1)\n",
    "                \n",
    "#                 if counter == n_samples:\n",
    "#                     print('counter reach {}'.format(n_samples))\n",
    "#                     break\n",
    "                    \n",
    "#             except IOError as e:\n",
    "#                 print('Skip!\\n')\n",
    "                \n",
    "\n",
    "#         TFWriter.close()\n",
    "#     print('Transform done!')\n",
    "#     print('total {} data processed'.format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file directory exist: True\n",
      "['/home3/user/myhsueh/h5_dataset/TD_26_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_31_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_74KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_22_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_20_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_24_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_85KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_41_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_71KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_78KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_23_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_19_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_81KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_47_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_88KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_82KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_87KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_86KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_44_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_60_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_90KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_56_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_35_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_75KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_58_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_33_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_1_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_8_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_96KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_9_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_98KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_11_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_92KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_38_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_13_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_70KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_4_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_7_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_66_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_40_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_16_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_3_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_14_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_62_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_39_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_6_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_61_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_10_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_64_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_52_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_17_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_69_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_30_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_80KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_93KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_45_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_28_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_65_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_95KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_29_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_46_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_37_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_72KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_59_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_76KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_32_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_43_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_34_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_83KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_63_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_21_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_51_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_99KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_25_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_48_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_42_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_50_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_79KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_84KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_91KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_55_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_94KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_49_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_73KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_53_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_18_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_97KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_89KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_54_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_5_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_0_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_68_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_27_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_15_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_2_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_12_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_57_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_67_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_77KB_clean.h5', '/home3/user/myhsueh/h5_dataset/TD_36_clean.h5']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# file directory\n",
    "TFR_ROOT ='/home3/user/myhsueh/h5_dataset/'\n",
    "print(\"file directory exist: {}\".format(os.path.exists(TFR_ROOT)))\n",
    "      \n",
    "# specify the filename      \n",
    "TFR_FILE_PATH = os.path.join(TFR_ROOT, '*_clean.h5')\n",
    "# TFR_FILE_PATH = os.path.join(TFR_ROOT, '*.h5')\n",
    "\n",
    "# use glob module to grab all the files in a list\n",
    "filenames = glob.glob(TFR_FILE_PATH)\n",
    "# filenames = filenames[:7]\n",
    "print(filenames)\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert(file_list, dir_path = '/home2/user/myhsueh/dataset/tfrecord/no_rir/training/'):\n",
    "    for h5_path_clean in file_list:\n",
    "        path, h5_file_clean = os.path.split(h5_path_clean)\n",
    "        filename_split = h5_file_clean.split('.')\n",
    "        h5_file_noisy =  filename_split[0].strip('clean') + 'noisy.' + filename_split[1]\n",
    "        h5_path_noisy = os.path.join(path, h5_file_noisy)\n",
    "        convert_to_TFRecord(h5_path_clean, h5_path_noisy, dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "# threads = 20\n",
    "\n",
    "# num_of_h5 = len(filenames)\n",
    "# # num_of_each_thread = int(num_of_h5*0.8) // threads\n",
    "# num_of_each_thread = int(num_of_h5) // threads\n",
    "# train_list = [filenames[i*num_of_each_thread:(i+1)*num_of_each_thread] for i in range(threads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pool = mp.Pool(processes=threads)\n",
    "# pool.map(convert, train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['/home3/user/myhsueh/h5_dataset/TD_3_clean.h5', \n",
    "             '/home3/user/myhsueh/h5_dataset/TD_11_clean.h5',\n",
    "             '/home3/user/myhsueh/h5_dataset/TD_13_clean.h5', \n",
    "             '/home3/user/myhsueh/h5_dataset/TD_14_clean.h5',\n",
    "             '/home3/user/myhsueh/h5_dataset/TD_16_clean.h5', \n",
    "             '/home3/user/myhsueh/h5_dataset/TD_34_clean.h5',\n",
    "             '/home3/user/myhsueh/h5_dataset/TD_38_clean.h5', \n",
    "             '/home3/user/myhsueh/h5_dataset/TD_39_clean.h5',\n",
    "             '/home3/user/myhsueh/h5_dataset/TD_43_clean.h5', \n",
    "             '/home3/user/myhsueh/h5_dataset/TD_50_clean.h5',\n",
    "             '/home3/user/myhsueh/h5_dataset/TD_62_clean.h5', \n",
    "             '/home3/user/myhsueh/h5_dataset/TD_63_clean.h5',\n",
    "             '/home3/user/myhsueh/h5_dataset/TD_79KB_clean.h5',\n",
    "             '/home3/user/myhsueh/h5_dataset/TD_83KB_clean.h5',\n",
    "             '/home3/user/myhsueh/h5_dataset/TD_84KB_clean.h5',\n",
    "             '/home3/user/myhsueh/h5_dataset/TD_91KB_clean.h5',\n",
    "             '/home3/user/myhsueh/h5_dataset/TD_92KB_clean.h5',\n",
    "             '/home3/user/myhsueh/h5_dataset/TD_98KB_clean.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start generate TD_3-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_3-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_3-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_3-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_11-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_11-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_11-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_11-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_13-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_13-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_13-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_13-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_14-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_14-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_14-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_14-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_16-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_16-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_16-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_16-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_34-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_34-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_34-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_34-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_38-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_38-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_38-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_38-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_39-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_39-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_39-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_39-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_43-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_43-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_43-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_43-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_50-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_50-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_50-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_50-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_62-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_62-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_62-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_62-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_63-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_63-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_63-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_63-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_79KB-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_79KB-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_79KB-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_79KB-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_83KB-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_83KB-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_83KB-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_83KB-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_84KB-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_84KB-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_84KB-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_84KB-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_91KB-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_91KB-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_91KB-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_91KB-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_92KB-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_92KB-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_92KB-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_92KB-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n",
      "\n",
      "Start generate TD_98KB-1-of-4.tfrecord\n",
      "\n",
      "Start generate TD_98KB-2-of-4.tfrecord\n",
      "\n",
      "Start generate TD_98KB-3-of-4.tfrecord\n",
      "\n",
      "Start generate TD_98KB-4-of-4.tfrecord\n",
      "counter reach 2000\n",
      "Transform done!\n",
      "total 2000 data processed\n"
     ]
    }
   ],
   "source": [
    "for i in filenames:\n",
    "    convert([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
