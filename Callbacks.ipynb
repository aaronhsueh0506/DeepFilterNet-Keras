{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e818ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4aa92c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:37: UserWarning: You are currently using a nightly version of TensorFlow (2.10.0-dev20220427). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input , Dense, Lambda, Normalization, Concatenate\n",
    "from tensorflow.keras.activations import relu, sigmoid, tanh\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, SGD, schedules\n",
    "from tensorflow_addons.optimizers import AdamW, extend_with_decoupled_weight_decay\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TerminateOnNaN, TensorBoard, \\\n",
    "                                        EarlyStopping, ReduceLROnPlateau, CSVLogger, Callback\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63cfa87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource\n",
    "from tensorflow.compat.v1 import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa74ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint(Callback):\n",
    "    def __init__(self, model, path):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.path = path\n",
    "        if not os.path.exists(path): os.makedirs(path)\n",
    "        self.best_loss = np.inf\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss = logs['val_loss']\n",
    "        if loss < self.best_loss:\n",
    "            print('Saving model to {}'.format(self.path.format(epoch=epoch, loss=loss)))\n",
    "            print(', Validation loss decreased from {} to {}.\\n'.format(self.best_loss, loss))\n",
    "            self.model.save_weights(self.path.format(epoch=epoch, loss=loss), overwrite=True)\n",
    "            self.best_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86dc4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearMemory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "        print('Memory usage: ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if int(K.get_value(self.model.optimizer.iterations))%50 == 0:\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68251791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCallback(Callback):\n",
    "    def __init__(self, callbacks_tensorboard, logdir):\n",
    "        self.tb_callback = callbacks_tensorboard\n",
    "        self.logdir = logdir\n",
    "        self.train_writer = tf.summary.create_file_writer(os.path.join(self.logdir,\"training\"))\n",
    "        self.val_writer = tf.summary.create_file_writer(os.path.join(self.logdir,\"validation\"))\n",
    "        \n",
    "        self.loss_tag = 'Epoch Summary/'\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        global_step = int(K.get_value(self.model.optimizer.iterations))\n",
    "        print('[INFO][BatchCallback] global step:{}'.format(global_step))\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "        logs.update({'global_step': K.get_value(self.model.optimizer.iterations)})\n",
    "        logs.update({'loss': logs['loss']})\n",
    "        logs.update({'maskloss': logs['maskloss']})\n",
    "        logs.update({'spectralloss': logs['spectralloss']})\n",
    "\n",
    "        self._write_log(logs)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_writer = self.train_writer\n",
    "        with train_writer.as_default():\n",
    "            for key, val in logs.items():\n",
    "                if key in ['loss']: tf.summary.scalar('loss', val, step=epoch)\n",
    "        train_writer.flush()\n",
    "        \n",
    "        val_writer = self.val_writer\n",
    "        with val_writer.as_default():\n",
    "            for key, val in logs.items():\n",
    "                if key in ['loss','val_loss']: tf.summary.scalar('loss', val, step=epoch)\n",
    "        val_writer.flush()\n",
    "        \n",
    "    def _write_log(self, logs):\n",
    "        writer = self.train_writer\n",
    "        with writer.as_default():\n",
    "            for key, val in logs.items():\n",
    "                if key in ['maskloss', 'dfalphaloss', 'spectralloss']:\n",
    "                    tag = 'Train(Metric)/' + key\n",
    "                    tf.summary.scalar(tag, val, step=logs['global_step'])\n",
    "                if key in ['loss']:\n",
    "                    tag = 'Train(Total)/' + key.upper()\n",
    "                    tf.summary.scalar(tag, val, step=logs['global_step'])\n",
    "                \n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbd6506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e612cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_decay_with_warmup(global_step,\n",
    "                             learning_rate_base,\n",
    "                             total_steps,\n",
    "                             warmup_learning_rate=0.0,\n",
    "                             warmup_steps=0,\n",
    "                             hold_base_rate_steps=0,\n",
    "                             steps_per_epoch=0):\n",
    "    \"\"\"Cosine decay schedule with warm up period.\n",
    "    Cosine annealing learning rate as described in:\n",
    "      Loshchilov and Hutter, SGDR: Stochastic Gradient Descent with Warm Restarts.\n",
    "      ICLR 2017. https://arxiv.org/abs/1608.03983\n",
    "    In this schedule, the learning rate grows linearly from warmup_learning_rate\n",
    "    to learning_rate_base for warmup_steps, then transitions to a cosine decay\n",
    "    schedule.\n",
    "    Arguments:\n",
    "        global_step {int} -- global step.\n",
    "        learning_rate_base {float} -- base learning rate.\n",
    "        total_steps {int} -- total number of training steps.\n",
    "    Keyword Arguments:\n",
    "        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n",
    "        warmup_steps {int} -- number of warmup steps. (default: {0})\n",
    "        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n",
    "                                    before decaying. (default: {0})\n",
    "    Returns:\n",
    "      a float representing learning rate.\n",
    "    Raises:\n",
    "      ValueError: if warmup_learning_rate is larger than learning_rate_base,\n",
    "        or if warmup_steps is larger than total_steps.\n",
    "    \"\"\"\n",
    "\n",
    "    if total_steps < warmup_steps:\n",
    "        raise ValueError('total_steps must be larger or equal to '\n",
    "                         'warmup_steps.')\n",
    "    learning_rate = 0.5 * learning_rate_base * (1 + np.cos(\n",
    "        np.pi *\n",
    "        (global_step - warmup_steps - hold_base_rate_steps\n",
    "         ) / float(total_steps - warmup_steps - hold_base_rate_steps)))\n",
    "#     learning_rate = learning_rate_base * 0.95 ** ((global_step - warmup_steps - hold_base_rate_steps) / (steps_per_epoch/25))\n",
    "    if hold_base_rate_steps > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold_base_rate_steps,\n",
    "                                 learning_rate, learning_rate_base)\n",
    "    if warmup_steps > 0:\n",
    "        if learning_rate_base < warmup_learning_rate:\n",
    "            raise ValueError('learning_rate_base must be larger or equal to '\n",
    "                             'warmup_learning_rate.')\n",
    "        slope = (learning_rate_base - warmup_learning_rate) / warmup_steps\n",
    "        warmup_rate = slope * global_step + warmup_learning_rate\n",
    "        learning_rate = np.where(global_step < warmup_steps, warmup_rate,\n",
    "                                 learning_rate)\n",
    "    return np.where(global_step > total_steps, 0.0, learning_rate)\n",
    "\n",
    "\n",
    "class WarmUpCosineDecayScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"Cosine decay with warmup learning rate scheduler\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate_base,\n",
    "                 total_steps,\n",
    "                 global_step_init=0,\n",
    "                 warmup_learning_rate=0.0,\n",
    "                 warmup_steps=0,\n",
    "                 hold_base_rate_steps=0,\n",
    "                 mini_lr=0,\n",
    "                 steps_per_epoch=0,\n",
    "                 verbose=0):\n",
    "        \"\"\"Constructor for cosine decay with warmup learning rate scheduler.\n",
    "    Arguments:\n",
    "        learning_rate_base {float} -- base learning rate.\n",
    "        total_steps {int} -- total number of training steps.\n",
    "    Keyword Arguments:\n",
    "        global_step_init {int} -- initial global step, e.g. from previous checkpoint.\n",
    "        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n",
    "        warmup_steps {int} -- number of warmup steps. (default: {0})\n",
    "        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n",
    "                                    before decaying. (default: {0})\n",
    "        verbose {int} -- 0: quiet, 1: update messages. (default: {0})\n",
    "        \"\"\"\n",
    "\n",
    "        super(WarmUpCosineDecayScheduler, self).__init__()\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.global_step = global_step_init\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.hold_base_rate_steps = hold_base_rate_steps\n",
    "        self.verbose = verbose\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.mini_lr = mini_lr\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.global_step = self.global_step + 1\n",
    "#         lr = K.get_value(self.model.optimizer.lr)\n",
    "#         self.learning_rates.append(lr)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = cosine_decay_with_warmup(global_step=self.global_step,\n",
    "                                      learning_rate_base=self.learning_rate_base,\n",
    "                                      total_steps=self.total_steps,\n",
    "                                      warmup_learning_rate=self.warmup_learning_rate,\n",
    "                                      warmup_steps=self.warmup_steps,\n",
    "                                      hold_base_rate_steps=self.hold_base_rate_steps,\n",
    "                                      steps_per_epoch=self.steps_per_epoch)\n",
    "        if lr < self.mini_lr: lr = self.mini_lr\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nBatch %05d: setting learning '\n",
    "                  'rate to %s.' % (self.global_step + 1, lr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
