{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e818ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4aa92c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input , Dense, Lambda, Normalization, Concatenate\n",
    "from tensorflow.keras.activations import relu, sigmoid, tanh\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, SGD, schedules\n",
    "from tensorflow_addons.optimizers import AdamW, extend_with_decoupled_weight_decay\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TerminateOnNaN, TensorBoard, \\\n",
    "                                        EarlyStopping, ReduceLROnPlateau, CSVLogger, Callback\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63cfa87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource\n",
    "from tensorflow.compat.v1 import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa74ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint(Callback):\n",
    "    def __init__(self, model, path, pre_loss = np.inf):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.path = path\n",
    "        if not os.path.exists(path): os.makedirs(path)\n",
    "        self.best_loss = pre_loss\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss = logs['val_loss']\n",
    "        if loss < self.best_loss:\n",
    "            print('\\nSaving model to {}'.format(self.path.format(epoch=epoch, loss=loss)))\n",
    "            print(', Validation loss decreased from {} to {}.\\n'.format(self.best_loss, loss))\n",
    "            self.model.save_weights(self.path.format(epoch=epoch, loss=loss), overwrite=True)\n",
    "            self.best_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86dc4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearMemory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "        print('Memory usage: ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if int(K.get_value(self.model.optimizer.iterations))%50 == 0:\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68251791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCallback(Callback):\n",
    "    def __init__(self, callbacks_tensorboard, logdir):\n",
    "        self.tb_callback = callbacks_tensorboard\n",
    "        self.logdir = logdir\n",
    "        self.train_writer = tf.summary.create_file_writer(os.path.join(self.logdir,\"training\"))\n",
    "        self.val_writer = tf.summary.create_file_writer(os.path.join(self.logdir,\"validation\"))\n",
    "        \n",
    "        self.loss_tag = 'Epoch Summary/'\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        global_step = int(K.get_value(self.model.optimizer.iterations))\n",
    "        print('[INFO][BatchCallback] global step:{}'.format(global_step))\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "        logs.update({'global_step': K.get_value(self.model.optimizer.iterations)})\n",
    "        logs.update({'loss': logs['loss']})\n",
    "        logs.update({'maskloss': logs['maskloss']})\n",
    "        logs.update({'spectralloss': logs['spectralloss']})\n",
    "\n",
    "        self._write_log(logs)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_writer = self.train_writer\n",
    "        with train_writer.as_default():\n",
    "            for key, val in logs.items():\n",
    "                if key in ['loss']: tf.summary.scalar('loss', val, step=epoch)\n",
    "        train_writer.flush()\n",
    "        \n",
    "        val_writer = self.val_writer\n",
    "        with val_writer.as_default():\n",
    "            for key, val in logs.items():\n",
    "                if key in ['loss','val_loss']: tf.summary.scalar('loss', val, step=epoch)\n",
    "        val_writer.flush()\n",
    "        \n",
    "    def _write_log(self, logs):\n",
    "        writer = self.train_writer\n",
    "        with writer.as_default():\n",
    "            for key, val in logs.items():\n",
    "                if key in ['maskloss', 'dfalphaloss', 'spectralloss']:\n",
    "                    tag = 'Train(Metric)/' + key\n",
    "                    tf.summary.scalar(tag, val, step=logs['global_step'])\n",
    "                if key in ['loss']:\n",
    "                    tag = 'Train(Total)/' + key.upper()\n",
    "                    tf.summary.scalar(tag, val, step=logs['global_step'])\n",
    "                \n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbd6506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e612cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_decay_with_warmup(global_step,\n",
    "                             learning_rate_base,\n",
    "                             total_steps,\n",
    "                             warmup_learning_rate=0.0,\n",
    "                             warmup_steps=0,\n",
    "                             hold_base_rate_steps=0,\n",
    "                             steps_per_epoch=0):\n",
    "    \"\"\"Cosine decay schedule with warm up period.\n",
    "    Cosine annealing learning rate as described in:\n",
    "      Loshchilov and Hutter, SGDR: Stochastic Gradient Descent with Warm Restarts.\n",
    "      ICLR 2017. https://arxiv.org/abs/1608.03983\n",
    "    In this schedule, the learning rate grows linearly from warmup_learning_rate\n",
    "    to learning_rate_base for warmup_steps, then transitions to a cosine decay\n",
    "    schedule.\n",
    "    Arguments:\n",
    "        global_step {int} -- global step.\n",
    "        learning_rate_base {float} -- base learning rate.\n",
    "        total_steps {int} -- total number of training steps.\n",
    "    Keyword Arguments:\n",
    "        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n",
    "        warmup_steps {int} -- number of warmup steps. (default: {0})\n",
    "        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n",
    "                                    before decaying. (default: {0})\n",
    "    Returns:\n",
    "      a float representing learning rate.\n",
    "    Raises:\n",
    "      ValueError: if warmup_learning_rate is larger than learning_rate_base,\n",
    "        or if warmup_steps is larger than total_steps.\n",
    "    \"\"\"\n",
    "\n",
    "    if total_steps < warmup_steps:\n",
    "        raise ValueError('total_steps must be larger or equal to '\n",
    "                         'warmup_steps.')\n",
    "    learning_rate = 0.5 * learning_rate_base * (1 + np.cos(\n",
    "        np.pi *\n",
    "        (global_step - warmup_steps - hold_base_rate_steps\n",
    "         ) / float(total_steps - warmup_steps - hold_base_rate_steps)))\n",
    "#     learning_rate = learning_rate_base * 0.95 ** ((global_step - warmup_steps - hold_base_rate_steps) / (steps_per_epoch/25))\n",
    "    if hold_base_rate_steps > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold_base_rate_steps,\n",
    "                                 learning_rate, learning_rate_base)\n",
    "    if warmup_steps > 0:\n",
    "        if learning_rate_base < warmup_learning_rate:\n",
    "            raise ValueError('learning_rate_base must be larger or equal to '\n",
    "                             'warmup_learning_rate.')\n",
    "        slope = (learning_rate_base - warmup_learning_rate) / warmup_steps\n",
    "        warmup_rate = slope * global_step + warmup_learning_rate\n",
    "        learning_rate = np.where(global_step < warmup_steps, warmup_rate,\n",
    "                                 learning_rate)\n",
    "    return np.where(global_step > total_steps, 0.0, learning_rate)\n",
    "\n",
    "\n",
    "class WarmUpCosineDecayScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"Cosine decay with warmup learning rate scheduler\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate_base,\n",
    "                 total_steps,\n",
    "                 global_step_init=0,\n",
    "                 warmup_learning_rate=0.0,\n",
    "                 warmup_steps=0,\n",
    "                 hold_base_rate_steps=0,\n",
    "                 mini_lr=0,\n",
    "                 steps_per_epoch=0,\n",
    "                 verbose=0):\n",
    "        \"\"\"Constructor for cosine decay with warmup learning rate scheduler.\n",
    "    Arguments:\n",
    "        learning_rate_base {float} -- base learning rate.\n",
    "        total_steps {int} -- total number of training steps.\n",
    "    Keyword Arguments:\n",
    "        global_step_init {int} -- initial global step, e.g. from previous checkpoint.\n",
    "        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n",
    "        warmup_steps {int} -- number of warmup steps. (default: {0})\n",
    "        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n",
    "                                    before decaying. (default: {0})\n",
    "        verbose {int} -- 0: quiet, 1: update messages. (default: {0})\n",
    "        \"\"\"\n",
    "\n",
    "        super(WarmUpCosineDecayScheduler, self).__init__()\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.global_step = global_step_init\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.hold_base_rate_steps = hold_base_rate_steps\n",
    "        self.verbose = verbose\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.mini_lr = mini_lr\n",
    "#         self.learning_rates = []\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.global_step = self.global_step + 1\n",
    "#         lr = K.get_value(self.model.optimizer.lr)\n",
    "#         self.learning_rates.append(lr)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = cosine_decay_with_warmup(global_step=self.global_step,\n",
    "                                      learning_rate_base=self.learning_rate_base,\n",
    "                                      total_steps=self.total_steps,\n",
    "                                      warmup_learning_rate=self.warmup_learning_rate,\n",
    "                                      warmup_steps=self.warmup_steps,\n",
    "                                      hold_base_rate_steps=self.hold_base_rate_steps,\n",
    "                                      steps_per_epoch=self.steps_per_epoch)\n",
    "        if lr < self.mini_lr: lr = self.mini_lr\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nBatch %05d: setting learning '\n",
    "                  'rate to %s.' % (self.global_step + 1, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079e842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCallback(Callback):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch>=5:\n",
    "            for layer in self.model.layers:\n",
    "                layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2caaf51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# # Create a model.\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_dim=100))\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "# model.compile(optimizer='rmsprop',\n",
    "#             loss='categorical_crossentropy',\n",
    "#             metrics=['accuracy'])\n",
    "\n",
    "# # Number of training samples.\n",
    "# sample_count = 100*32\n",
    "\n",
    "# # Total epochs to train.\n",
    "# epochs = 30\n",
    "\n",
    "# # Number of warmup epochs.\n",
    "# warmup_epoch = 2\n",
    "\n",
    "# # Training batch size, set small value here for demonstration purpose.\n",
    "# batch_size = 32\n",
    "\n",
    "# # Base learning rate after warmup.\n",
    "# learning_rate_base = 0.001\n",
    "\n",
    "# total_steps = int(epochs * sample_count / batch_size)\n",
    "\n",
    "# # Compute the number of warmup batches.\n",
    "# warmup_steps = int(warmup_epoch * sample_count / batch_size)\n",
    "\n",
    "# # Generate dummy data.\n",
    "# data = np.random.random((sample_count, 100))\n",
    "# labels = np.random.randint(10, size=(sample_count, 1))\n",
    "\n",
    "# # Convert labels to categorical one-hot encoding.\n",
    "# one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# # Compute the number of warmup batches.\n",
    "# warmup_batches = warmup_epoch * sample_count / batch_size\n",
    "\n",
    "# # Create the Learning rate scheduler.\n",
    "# warm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base=1e-4,\n",
    "#                                         total_steps=epochs*100,\n",
    "#                                         warmup_steps=1*100,\n",
    "#                                         warmup_learning_rate = 1e-4,\n",
    "#                                         mini_lr=1e-6,\n",
    "#                                         steps_per_epoch=100,\n",
    "#                                         global_step_init=0*100,\n",
    "#                                         verbose = 1)\n",
    "\n",
    "# # Train the model, iterating on the data in batches of 32 samples\n",
    "# model.fit(data, one_hot_labels, epochs=epochs, batch_size=batch_size,\n",
    "#         verbose=0, callbacks=[warm_up_lr])\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(warm_up_lr.learning_rates)\n",
    "# plt.xlabel('Step', fontsize=20)\n",
    "# plt.ylabel('lr', fontsize=20)\n",
    "# plt.axis([0, total_steps, 0, learning_rate_base*1.1])\n",
    "# plt.xticks(np.arange(0, total_steps, 50))\n",
    "# plt.grid()\n",
    "# plt.title('Cosine decay with warmup', fontsize=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51f248f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001000 0.0004000 0.0007000 0.0010000 0.0009966 0.0009865 0.0009698 0.0009468 0.0009177 0.0008830 \n",
      "0.0008431 0.0007986 0.0007500 0.0006980 0.0006434 0.0005868 0.0005291 0.0004709 0.0004132 0.0003566 \n",
      "0.0003020 0.0002500 0.0002014 0.0001569 0.0001170 0.0000823 0.0000532 0.0000302 0.0000135 0.0000034 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(3.38082113e-06)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lr = []\n",
    "# for i in range(30):\n",
    "#     lr.append(cosine_decay_with_warmup(global_step=i,\n",
    "#                               learning_rate_base=1e-3,\n",
    "#                               total_steps=30*1,\n",
    "#                               warmup_learning_rate=1e-4,\n",
    "#                               warmup_steps=3*1,\n",
    "#                               hold_base_rate_steps=0,\n",
    "#                               steps_per_epoch=1))\n",
    "# for i in range(3):\n",
    "#     for j in range(10):\n",
    "#         print('%.7f'%lr[i*10+j], end=' ')\n",
    "#     print()\n",
    "# lr[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70dadc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00e-04 4.00e-04 7.00e-04 1.00e-03 9.97e-04 9.87e-04 9.70e-04 9.47e-04\n",
      " 9.18e-04 8.83e-04 8.43e-04 7.99e-04 7.50e-04 6.98e-04 6.43e-04 5.87e-04\n",
      " 5.29e-04 4.71e-04 4.13e-04 3.57e-04 3.02e-04 2.50e-04 2.01e-04 1.57e-04\n",
      " 1.17e-04 8.20e-05 5.30e-05 3.00e-05 1.30e-05 3.00e-06]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6c5757a430>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(np.round(lr,6))\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9b4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
