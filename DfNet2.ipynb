{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934aac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c677106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input , Dense, Lambda, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.activations import relu, sigmoid, tanh\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, SGD, schedules\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TerminateOnNaN, TensorBoard, \\\n",
    "                                        EarlyStopping, ReduceLROnPlateau, CSVLogger, Callback\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3141967",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "\n",
    "from modules import convkxf, GroupGRULayer, GroupFC\n",
    "from loss import MaskLoss, LocalSnrTarget, DfAlphaLoss, SpectralLoss, SISNR_Loss\n",
    "from utils import mask_operations, df_operations, synthesis_frame, df_operations_wo_alpha\n",
    "from params import model_params\n",
    "from dataloader import read_tfrecod_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff6bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Callbacks import WarmUpCosineDecayScheduler, BatchCallback, ClearMemory, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38dff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.compat.v1 import ConfigProto, InteractiveSession, enable_eager_execution\n",
    "\n",
    "# config = ConfigProto(allow_soft_placement=True)\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "# config.gpu_options.allow_growth = True\n",
    "# InteractiveSession(config=config)\n",
    "# enable_eager_execution(config=config)\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e15be",
   "metadata": {},
   "source": [
    "# Read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model_params('config.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e9a85",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818436e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home2/user/myhsueh/dataset/tfrecord/'\n",
    "\n",
    "training_set_path = [\n",
    "    dataset_path + 'rir/training', \n",
    "    dataset_path + 'no_rir/training',\n",
    "    dataset_path + 'rir/KB/training', \n",
    "    dataset_path + 'no_rir/KB/training',\n",
    "    dataset_path + 'no_rir/KB2/training'\n",
    "]\n",
    "validation_set_path = [\n",
    "    dataset_path + 'rir/validation', \n",
    "    dataset_path + 'no_rir/validation',\n",
    "    dataset_path + 'rir/KB/validation', \n",
    "    dataset_path + 'no_rir/KB/validation',\n",
    "    dataset_path + 'no_rir/KB2/validation'\n",
    "]\n",
    "\n",
    "combine_train, combine_val = False, False\n",
    "if len(training_set_path) > 1 : combine_train = True\n",
    "if len(validation_set_path) > 1 : combine_val = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ccd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_tfrecod_data(training_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efbb3db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_dataset = read_tfrecod_data(validation_set_path, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dff8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "each_tfr_batch = 6000 // 4 // p.length_sec\n",
    "train_count, val_count = 0, 0\n",
    "if combine_train:\n",
    "    for i in training_set_path:\n",
    "        train_count += len(os.listdir(i))\n",
    "else:\n",
    "    train_count = len(os.listdir(training_set_path[0]))\n",
    "    \n",
    "if combine_val:\n",
    "    for i in validation_set_path:\n",
    "        val_count += len(os.listdir(i))\n",
    "else:\n",
    "    val_count = len(os.listdir(validation_set_path[0]))\n",
    "\n",
    "steps_per_epoch = train_count*each_tfr_batch//p.batch_size \n",
    "validation_steps = val_count*each_tfr_batch//p.batch_size \n",
    "print('training step: {} , validation step: {}'.format(steps_per_epoch,validation_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd863e94",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcef9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BN_type = \"normal\" # range normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "erb_inputs = Input(shape=(None, p.nb_erb, 1), name='ERB_input')\n",
    "spec_inputs = Input(shape=(None, p.nb_df, 2), name='spec_input') # complex\n",
    "clean_spec = Input(shape=(None, p.fft_size//2+1, 2), name='clean_spec') # complex\n",
    "noisy_spec = Input(shape=(None, p.fft_size//2+1, 2), name='noisy_spec') # complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572e9ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "erb_conv0 = convkxf(erb_inputs, p.conv_out_ch, k=3, f=3, fstride=1, bias=False, batch_norm=True, \n",
    "                    training=True, infer=True, BN_type = BN_type, name='conv0_encoder')\n",
    "erb_conv1 = convkxf(erb_conv0, p.conv_out_ch, k=1, f=3, fstride=2, bias=False, batch_norm=True,\n",
    "                    infer=True, BN_type = BN_type, name='conv1_encoder')\n",
    "erb_conv2 = convkxf(erb_conv1, p.conv_out_ch, k=1, f=3, fstride=2, bias=False, batch_norm=True,\n",
    "                    infer=True, BN_type = BN_type, name='conv2_encoder')\n",
    "erb_conv3 = convkxf(erb_conv2, p.conv_out_ch, k=1, f=3, fstride=1, bias=False, batch_norm=True,\n",
    "                    infer=True, BN_type = BN_type, name='conv3_encoder')\n",
    "\n",
    "df_conv0 = convkxf(spec_inputs, p.conv_out_ch, k=3, f=3, fstride=1, batch_norm=True, training=True,\n",
    "                   infer=True, BN_type = BN_type, name='df_conv0_encoder')\n",
    "df_conv1 = convkxf(df_conv0, p.conv_out_ch, k=1, f=3, fstride=2, batch_norm=True,\n",
    "                   infer=True, BN_type = BN_type, name='df_conv1_encoder')\n",
    "\n",
    "shape = [tf.shape(df_conv1)[l] for l in range(4)]\n",
    "df_conv1 = tf.transpose(df_conv1,(0,1,3,2))\n",
    "df_conv1 = tf.reshape(df_conv1, [shape[0], shape[1], shape[2]*shape[3]])\n",
    "\n",
    "cemb = GroupFC(df_conv1, p.fc_hidden, p.fc_group, infer=True, name='GFC_encoder')\n",
    "    \n",
    "shape = [tf.shape(erb_conv3)[l] for l in range(4)]\n",
    "emb = Concatenate()([cemb, tf.reshape(tf.transpose(erb_conv3,(0,1,3,2)), [shape[0], shape[1], shape[2]*shape[3]])])\n",
    "\n",
    "GRU_emb = GroupGRULayer(emb, p.gru_hidden, p.gru_group, num_layer=1, name='GGRU0', add_output=True, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ebf65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_emb1 = GroupGRULayer(GRU_emb, p.gru_hidden, p.gru_group, num_layer=2, name='GGRU01', add_output=True, norm=True)\n",
    "# GRU_emb1 = BatchNormalization()(GRU_emb1)\n",
    "de_emb = GroupFC(GRU_emb1, p.fc_hidden, p.fc_group, activation='relu', infer=True, name='GFC_decoder', norm=True)\n",
    "\n",
    "shape = [tf.shape(erb_conv3)[l] for l in range(4)]\n",
    "# shape = erb_conv3.get_shape().as_list()\n",
    "emb_decoder = tf.reshape(de_emb, shape = [shape[0], shape[1], shape[2], shape[3]], name='decoder_reshape')\n",
    "\n",
    "kwargs = {\n",
    "    \"k\": 1,\n",
    "    \"batch_norm\": True,\n",
    "    \"BN_type\": BN_type,\n",
    "    \"infer\": True\n",
    "}\n",
    "tkwargs = {\n",
    "    \"k\": 1,\n",
    "    \"batch_norm\": True,\n",
    "    \"mode\": \"transposed\",\n",
    "    \"BN_type\": BN_type,\n",
    "    \"infer\": True\n",
    "}\n",
    "pkwargs = {\n",
    "    \"k\": 1,\n",
    "    \"f\": 1,\n",
    "    \"batch_norm\": True,\n",
    "    \"BN_type\": BN_type,\n",
    "    \"infer\": True\n",
    "}\n",
    "\n",
    "convp3 = convkxf(erb_conv3, out_ch=p.conv_out_ch, name='convp3', **pkwargs)  # Conv\n",
    "vt3_in = convp3 + emb_decoder\n",
    "convt3 = convkxf(vt3_in, out_ch=p.conv_out_ch, fstride=1, name='convt3', **kwargs) #ConvT\n",
    "\n",
    "convp2 = convkxf(erb_conv2, out_ch=p.conv_out_ch, name='convp2', **pkwargs) # Conv\n",
    "vt2_in = convp2 + convt3\n",
    "convt2 = convkxf(vt2_in, out_ch=p.conv_out_ch, name='convt2', **tkwargs)\n",
    "\n",
    "convp1 = convkxf(erb_conv1, out_ch=p.conv_out_ch, name='convp1', **pkwargs) # Conv\n",
    "vt1_in = convp1 + convt2\n",
    "convt1 = convkxf(vt1_in, out_ch=p.conv_out_ch, name='convt1', **tkwargs)\n",
    "\n",
    "convp0 = convkxf(erb_conv0, out_ch=p.conv_out_ch, name='convp0', **pkwargs) # Conv\n",
    "vt0_in = convp0 + convt1\n",
    "mask_out = convkxf(vt0_in, out_ch=1, k=1, fstride=1, act='sigmoid', batch_norm=False, \n",
    "                   reshape=True, name='mask_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18193a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_emb2 = GroupGRULayer(GRU_emb, p.gru_hidden, p.gru_group, num_layer=2, name='GGRU1', add_output=True, norm=True)\n",
    "# GRU_emb2 = BatchNormalization()(GRU_emb2)\n",
    "convp = convkxf(df_conv0, 2*p.df_order, k=1, f=1, complex_in=True, batch_norm=True, \n",
    "                BN_type = BN_type, infer=True, name='convp_DfDecoder') \n",
    "convp = tf.transpose(convp, (0,1,3,2))\n",
    "\n",
    "df_alpha = Dense(1, name='convp_alpha', activation='sigmoid', #)(GRU_emb2)\n",
    "                        kernel_constraint= constraint, \n",
    "                        bias_constraint= constraint)(GRU_emb2)\n",
    "# c = Dense(p.nb_df*p.df_order*2, name='convp_c', activation='tanh')(GRU_emb2)\n",
    "#                         kernel_constraint= constraint, \n",
    "#                         bias_constraint= constraint)(GRU_emb2)\n",
    "c = GroupFC(GRU_emb2, p.nb_df*p.df_order*2, p.fc_group, activation='tanh', name='GFC_c')\n",
    "\n",
    "shape = [tf.shape(c)[k] for k in range(2)]\n",
    "c = tf.reshape(c,(shape[0], shape[1], p.df_order*2, p.nb_df))\n",
    "\n",
    "c = tf.reshape(c + convp,(shape[0], shape[1], p.df_order, 2, p.nb_df))\n",
    "\n",
    "df_coeff = tf.transpose(c, [0, 1, 2, 4, 3], name='output_c')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38180c11",
   "metadata": {},
   "source": [
    "# Enhance operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11541142",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = mask_operations(noisy_spec, mask_out) # mask gain\n",
    "enhanced = df_operations(spec, df_coeff, df_alpha) # deep filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c397d",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19617a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_nan = TerminateOnNaN()\n",
    "callbacks_earlystop = EarlyStopping(patience=5, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d405156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "logdir = \"./logs_6\"\n",
    "callbacks_tensorboard = TensorBoard(log_dir=logdir)\n",
    "tb_callback = BatchCallback(callbacks_tensorboard, logdir)\n",
    "\n",
    "# Memory clear\n",
    "Memory_callback=ClearMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5d168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "037927ff",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e425a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maskloss = Lambda(lambda x:MaskLoss(*x, factor=p.mask_factor, r=p.mask_gamma), \n",
    "                  name='maskloss')([mask_out, clean_spec, noisy_spec])\n",
    "\n",
    "spectralloss = Lambda(lambda x:SpectralLoss(*x, gamma=p.df_gamma, factor_mag=p.df_factor, factor_img=p.df_factor), \n",
    "                      name='spectralloss')([enhanced, clean_spec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d2b67",
   "metadata": {},
   "source": [
    "# Model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ef4ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs = [erb_inputs, spec_inputs, clean_spec, noisy_spec]\n",
    "outputs = [maskloss, spectralloss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f43f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=inputs, outputs=outputs, name='DfNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39435d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fb1ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf44364",
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base=p.lr,\n",
    "                                        total_steps=p.epochs*steps_per_epoch,\n",
    "                                        warmup_learning_rate=0.0,\n",
    "                                        warmup_steps=2*steps_per_epoch,\n",
    "                                        hold_base_rate_steps=0,\n",
    "                                        mini_lr=1e-7,\n",
    "                                        steps_per_epoch=steps_per_epoch,\n",
    "                                        global_step_init=0*steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "cp_callback = ModelCheckpoint(model, './weights2_6/weights.{epoch:02d}-{loss:.2f}.h5')\n",
    "callbacks_overall = [tb_callback, cp_callback, warm_up_lr,\n",
    "                     callbacks_nan, callbacks_earlystop, Memory_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93303658",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_metric(maskloss, name = \"maskloss\")\n",
    "model.add_metric(spectralloss, name = \"spectralloss\")\n",
    "model.add_loss(spectralloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(learning_rate=p.lr, weight_decay=0.0, clipnorm=1)\n",
    "# optimizer = Adam(learning_rate=p.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ed691",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, run_eagerly=True) #, loss_weights={\"dfalphaloss\": 1, \"spectralloss\": 20}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500705ec",
   "metadata": {},
   "source": [
    "# Pre-train weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ecac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5_lists = os.listdir(\"./weights2_6\")\n",
    "# h5_lists.sort(key=lambda fn:os.path.getmtime(\"./weights2_6/\" + fn)\n",
    "#                 if not os.path.isdir(\"./weights2_6/\" + fn) else 0)\n",
    "# new_file = os.path.join(\"./weights2_6/\", h5_lists[-1])\n",
    "# print(new_file)\n",
    "\n",
    "# model.load_weights(new_file, by_name=True)\n",
    "# model.save(\"weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# keys = []\n",
    "# with h5py.File(new_file,'r') as f: # open file\n",
    "#     f.visit(keys.append) # append all keys to list\n",
    "#     for key in keys:\n",
    "#         if ':' in key: # contains data if ':' in key\n",
    "#             print(f[key].name)\n",
    "    \n",
    "# f = h5py.File(new_file,'r')\n",
    "# group = f[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c600a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_param(folding_layer, weight_name):\n",
    "#     layer = '/' + folding_layer + '/' + folding_layer + '/'\n",
    "#     if folding_layer == 'GGRU00': weight_name = 'gru_cell/'+ weight_name\n",
    "#     if folding_layer == 'GGRU010': weight_name = 'gru_cell_1/'+ weight_name\n",
    "#     if folding_layer == 'GGRU011': weight_name = 'gru_cell_2/'+ weight_name\n",
    "#     if folding_layer == 'GGRU10': weight_name = 'gru_cell_3/'+ weight_name\n",
    "#     if folding_layer == 'GGRU11': weight_name = 'gru_cell_4/'+ weight_name\n",
    "#     return group[layer+weight_name+':0'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18dc832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store weights before loading pre-trained weights\n",
    "# preloaded_layers = model.layers.copy()\n",
    "# preloaded_weights = []\n",
    "# for pre in preloaded_layers:\n",
    "#     preloaded_weights.append(pre.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a936c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     if layer.name[:4] == 'GGRU':\n",
    "#         kernel_weight = get_param(layer.name, 'kernel')\n",
    "#         recurrent_weights = get_param(layer.name, 'recurrent_kernel')\n",
    "#         bias = get_param(layer.name, 'bias')\n",
    "#         layer.set_weights([kernel_weight, recurrent_weights, bias])\n",
    "#     else:\n",
    "#         try:\n",
    "#             kernel_weight = get_param(layer.name, 'kernel')\n",
    "#             try:\n",
    "#                 bias = get_param(layer.name, 'bias')\n",
    "#                 layer.set_weights([kernel_weight, bias])\n",
    "#             except:\n",
    "#                 layer.set_weights([kernel_weight])\n",
    "#         except:\n",
    "#              if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "#                 gamma = get_param(layer.name, 'gamma')\n",
    "#                 beta = get_param(layer.name, 'beta')\n",
    "#                 moving_mean = get_param(layer.name, 'moving_mean')\n",
    "#                 moving_variance = get_param(layer.name, 'moving_variance') \n",
    "#                 layer.set_weights([gamma, beta, moving_mean, moving_variance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac8c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # store weights before loading pre-trained weights\n",
    "# # preloaded_layers = model.layers.copy()\n",
    "# # preloaded_weights = []\n",
    "# # for pre in preloaded_layers:\n",
    "# #     preloaded_weights.append(pre.get_weights())\n",
    "\n",
    "# # load pre-trained weights\n",
    "# # model.load_weights(new_file, by_name=True)\n",
    "\n",
    "# # # compare previews weights vs loaded weights\n",
    "# for layer, pre in zip(model.layers, preloaded_weights):\n",
    "#     weights = layer.get_weights()\n",
    "\n",
    "#     if weights:\n",
    "#         if np.array_equal(weights, pre):\n",
    "#             print('not loaded', layer.name)\n",
    "#         else:\n",
    "#             print('loaded', layer.name)\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f197a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d1bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset,\n",
    "                    epochs=p.epochs,\n",
    "                    validation_data=val_dataset,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=callbacks_overall,\n",
    "                    initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f3eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4_1 continue training w/ batch 24\n",
    "# 4_2 continue training w/ batch 32 w/o constriant\n",
    "\n",
    "# 5 new training bias=False for convkxf\n",
    "# 6 new trainging w/ BN for FC and GRU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
