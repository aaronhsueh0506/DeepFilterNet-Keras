{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Conv2D, ZeroPadding2D, Conv2DTranspose, BatchNormalization, SeparableConv2D\n",
    "from tensorflow.keras.activations import elu, relu, sigmoid, tanh\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import HeUniform\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = l2(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_conv2d_transpose(inputs, groups, convkwargs):\n",
    "    \"\"\"Performs grouped transposed convolution.\n",
    "\n",
    "    Args:\n",
    "        inputs: A `Tensor` of shape `[batch_size, h, w, c]`.\n",
    "        filters: The number of convolutional filters.\n",
    "        kernel_size: The spatial size of the convolutional kernel.\n",
    "        strides: The convolutional stride.\n",
    "        groups: The number of groups to use in the grouped convolution step.\n",
    "            The input channel count needs to be evenly divisible by `groups`.\n",
    "    Returns:\n",
    "        A `Tensor` of shape `[batch_size, new_h, new_w, filters]`.\n",
    "    \"\"\"\n",
    "    splits = tf.split(inputs, groups, axis=-1)\n",
    "    convolved_splits = [\n",
    "        Conv2DTranspose(**convkwargs)(split) for split in splits\n",
    "    ]\n",
    "    return tf.concat(convolved_splits, -1)\n",
    "\n",
    "def grouped_conv2d(inputs, groups, convkwargs):\n",
    "    \n",
    "    splits = tf.split(inputs, groups, axis=-1)\n",
    "    convolved_splits = [\n",
    "        Conv2D(**convkwargs)(split) for split in splits\n",
    "    ]\n",
    "    return tf.concat(convolved_splits, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RELU(x):\n",
    "    return tf.where(x<0.0, 0.0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convkxf_old(inputs,\n",
    "            out_ch: int,\n",
    "            k: int = 1,\n",
    "            f: int = 3,\n",
    "            fstride: int = 2,\n",
    "            lookahead: int = 0,\n",
    "            batch_norm: bool = False,\n",
    "            act = 'relu',\n",
    "            mode = \"normal\",\n",
    "            depthwise: bool = True,\n",
    "            complex_in: bool = False,\n",
    "            reshape: bool = False,\n",
    "            name: str = 'conv',\n",
    "            training=True\n",
    "           ):\n",
    "    in_ch = inputs.get_shape()[-1]\n",
    "    bias = batch_norm is False\n",
    "    stride = 1 if f == 1 else (1, fstride)\n",
    "    fpad = (f - 1) // 2\n",
    "#     convpad = (0, fpad)\n",
    "        \n",
    "    if depthwise: groups = min(in_ch, out_ch)\n",
    "    else: groups = 1\n",
    "        \n",
    "    if in_ch % groups != 0 or out_ch % groups != 0: groups = 1    \n",
    "    if complex_in and groups % 2 == 0: groups //= 2\n",
    "        \n",
    "    convkwargs = {\n",
    "        \"filters\": out_ch//groups,\n",
    "        \"kernel_size\": (k, f),\n",
    "        \"strides\": stride,\n",
    "        \"use_bias\": bias,\n",
    "        \"padding\": 'same',\n",
    "#         \"name\": name,\n",
    "    }\n",
    "    \n",
    "    if mode == \"normal\":\n",
    "        if not training:\n",
    "            convkwargs = {\"filters\": out_ch//groups,\n",
    "                          \"kernel_size\": (k, f),\n",
    "                          \"strides\": stride,\n",
    "                          \"use_bias\": bias,\n",
    "                          \"padding\": 'valid'}\n",
    "            if fpad>0:\n",
    "                paddings = tf.constant([[0,0],[0,0],[fpad,fpad],[0,0]])\n",
    "                inputs = tf.pad(inputs, paddings, \"CONSTANT\", constant_values=0)\n",
    "        \n",
    "        if groups>1: conv_out = grouped_conv2d(inputs, groups, convkwargs)\n",
    "        else: conv_out = grouped_conv2d(inputs, groups, convkwargs)\n",
    "        \n",
    "    elif mode == \"transposed\":\n",
    "        if groups>1: conv_out = grouped_conv2d_transpose(inputs, groups, convkwargs)\n",
    "        else: conv_out = Conv2DTranspose(**convkwargs)(inputs)        \n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    print(convkwargs)\n",
    "    \n",
    "    if groups>1: \n",
    "        conv_out = Conv2D(out_ch, kernel_size=1, trainable = training,\n",
    "                          use_bias=False, name=name + '_1x1')(conv_out)\n",
    "    if batch_norm: \n",
    "        conv_out = BatchNormalization(momentum=0.1, epsilon=1e-5, name=name+'BN')(conv_out)\n",
    "        \n",
    "    if act == 'relu': \n",
    "        conv_out = RELU(conv_out)\n",
    "    elif act == 'sigmoid': conv_out = sigmoid(conv_out)\n",
    "    else: pass\n",
    "    \n",
    "    if reshape: \n",
    "        conv_out = tf.squeeze(conv_out,-1)\n",
    "        \n",
    "    return conv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convkxf(inputs,\n",
    "            out_ch: int,\n",
    "            k: int = 1,\n",
    "            f: int = 3,\n",
    "            fstride: int = 2,\n",
    "            lookahead: int = 0,\n",
    "            batch_norm: bool = False,\n",
    "            act = 'relu',\n",
    "            mode = \"normal\",\n",
    "            depthwise: bool = True,\n",
    "            complex_in: bool = False,\n",
    "            reshape: bool = False,\n",
    "            name: str = 'conv',\n",
    "            training=True,\n",
    "            bias: bool = True,\n",
    "           ):\n",
    "    in_ch = inputs.get_shape()[-1]\n",
    "#     bias = batch_norm is False\n",
    "    stride = 1 if f == 1 else (1, fstride)\n",
    "    fpad = (f - 1) // 2 # freq\n",
    "        \n",
    "    if depthwise: groups = min(in_ch, out_ch)\n",
    "    else: groups = 1\n",
    "        \n",
    "    if in_ch % groups != 0 or out_ch % groups != 0: groups = 1    \n",
    "    if complex_in and groups % 2 == 0: groups //= 2\n",
    "    \n",
    "    if mode == \"normal\":\n",
    "        convkwargs = {\n",
    "#         \"filters\": out_ch//groups,\n",
    "        \"filters\": out_ch,\n",
    "        \"groups\": groups,\n",
    "        \"kernel_size\": (k, f),\n",
    "        \"strides\": stride,\n",
    "        \"use_bias\": bias,\n",
    "        \"padding\": 'valid',\n",
    "        \"kernel_initializer\": 'he_normal',\n",
    "        \"name\": name\n",
    "        }\n",
    "        if training:\n",
    "            paddings = tf.constant([[0,0],[k-1-lookahead,lookahead],[0,0],[0,0]])\n",
    "            pad_flag = (0, 0, k - 1 - lookahead, lookahead)\n",
    "            if any(p > 0 for p in pad_flag):\n",
    "                inputs = tf.pad(inputs, paddings, \"CONSTANT\", constant_values=0)\n",
    "            \n",
    "        if fpad>0:\n",
    "            paddings = tf.constant([[0,0],[0,0],[fpad,fpad],[0,0]])\n",
    "            inputs = tf.pad(inputs, paddings, \"CONSTANT\", constant_values=0)\n",
    "#         if groups>1: \n",
    "#             conv_out = grouped_conv2d(inputs, groups, convkwargs)\n",
    "#         else: conv_out = Conv2D(**convkwargs)(inputs)\n",
    "        conv_out = Conv2D(**convkwargs)(inputs)\n",
    "        \n",
    "    elif mode == \"transposed\":\n",
    "        convkwargs = {\n",
    "        \"filters\": out_ch//groups,\n",
    "        \"kernel_size\": (k, f),\n",
    "        \"strides\": stride,\n",
    "        \"use_bias\": bias,\n",
    "        \"padding\": 'same',\n",
    "        \"kernel_initializer\": 'he_normal',\n",
    "        }\n",
    "        if groups>1: conv_out = grouped_conv2d_transpose(inputs, groups, convkwargs)\n",
    "        else: conv_out = Conv2DTranspose(**convkwargs)(inputs) \n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "            \n",
    "    if groups>1: \n",
    "        conv_out = Conv2D(out_ch, kernel_size=1, trainable = training,\n",
    "                          use_bias=False, name=name + '_1x1')(conv_out)\n",
    "    if batch_norm: \n",
    "        conv_out = BatchNormalization(momentum=0.1, epsilon=1e-5)(conv_out)\n",
    "        \n",
    "    if act == 'relu': \n",
    "        conv_out = RELU(conv_out)\n",
    "    elif act == 'sigmoid': conv_out = sigmoid(conv_out)\n",
    "    else: pass\n",
    "    \n",
    "    if reshape: \n",
    "        conv_out = tf.squeeze(conv_out,-1)\n",
    "        \n",
    "    return conv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GroupGRU(inputs, hidden, groups, return_sequences=True, name='GGRU', reshape=True, count=0, training=True):\n",
    "    groups_inputs = tf.split(inputs, groups, axis=-1)\n",
    "    gru_list = []\n",
    "    grukwargs = {\n",
    "        \"units\": hidden//groups,\n",
    "        \"return_sequences\": True,\n",
    "        \"kernel_initializer\": 'he_normal',\n",
    "    }\n",
    "    for group_inputs in groups_inputs: # group number\n",
    "        if not training: gru_tmp = GRU(**grukwargs, stateful=True, name=name + '_' +str(count))(group_inputs)\n",
    "        else: gru_tmp = GRU(**grukwargs, name=name + '_' +str(count))(group_inputs)\n",
    "        gru_list.append(gru_tmp)\n",
    "        count+=1\n",
    "    gru_output = tf.stack(gru_list, axis=-1, name=name + '_stack')\n",
    "    if reshape:\n",
    "        gru_output = tf.transpose(gru_output, [0, 1, 3, 2], name=name + '_transpose')\n",
    "    \n",
    "    shape = [tf.shape(gru_output)[l] for l in range(4)]\n",
    "#     shape = gru_output.get_shape().as_list()\n",
    "    gru_layer_output = tf.reshape(gru_output, shape = [shape[0], shape[1], shape[-1]*shape[-2]], name=name + '_reshape')\n",
    "    \n",
    "    if return_sequences:\n",
    "        return gru_layer_output\n",
    "    else:\n",
    "        return gru_layer_output[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GroupGRULayer(inputs, hidden, groups, num_layer, name='GGRU', add_output=True, training=True):\n",
    "    for i in range(num_layer):\n",
    "        if i < num_layer-1: reshape = True\n",
    "        else: reshape = False\n",
    "        \n",
    "        inputs = GroupGRU(inputs, hidden, groups, return_sequences=True, name=name + str(i), \\\n",
    "                          reshape=reshape, training=training)\n",
    "        if add_output:\n",
    "            if i == 0 : output = inputs\n",
    "            else: output += inputs\n",
    "                \n",
    "        else: output = inputs\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GroupFC(inputs, hidden, groups=8, activation=None, name='GFC', count=0):\n",
    "    groups_inputs = tf.split(inputs, groups, axis=-1)\n",
    "    FC_list = []\n",
    "    fckwargs = {\n",
    "    \"units\": hidden//groups,\n",
    "    \"kernel_initializer\": 'he_normal',\n",
    "    }\n",
    "    for group_inputs in groups_inputs: # group number\n",
    "        FC_tmp = Dense(**fckwargs, name=name + '_' +str(count))(group_inputs)\n",
    "        FC_list.append(FC_tmp)\n",
    "        count+=1\n",
    "    FC_output = tf.stack(FC_list, axis=-1, name=name + '_stack')\n",
    "    rearange = tf.transpose(FC_output, [0, 1, 3, 2], name=name + '_transpose')\n",
    "    \n",
    "    shape = [tf.shape(rearange)[l] for l in range(4)]\n",
    "#     shape = rearange.get_shape().as_list()\n",
    "    FC_layer_output = tf.reshape(rearange, shape = [shape[0], shape[1], shape[-1]*shape[-2]], name=name + '_reshape')\n",
    "    \n",
    "    if activation == 'relu': \n",
    "        FC_layer_output = RELU(FC_layer_output)\n",
    "#         FC_layer_output = tf.where(FC_layer_output < 0, 0.0, FC_layer_output)\n",
    "    elif activation == 'sigmoid': FC_layer_output = sigmoid(FC_layer_output)\n",
    "    else: FC_layer_output = FC_layer_output\n",
    "        \n",
    "    return FC_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
